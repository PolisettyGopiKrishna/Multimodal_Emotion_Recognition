# Multimodal Emotion Recognition

## ğŸ“Œ Objective

This project builds a Multimodal Emotion Recognition system using:

- ğŸ¤ Speech only
- ğŸ“ Text only
- ğŸ”— Fusion of Speech + Text

The goal is to compare the performance of unimodal and multimodal systems using the Toronto Emotional Speech Set (TESS).

---

## ğŸ“‚ Dataset

**Dataset Used:** Toronto Emotional Speech Set (TESS)

The dataset contains:
- Speech audio files (.wav)
- Corresponding transcript words (derived from filename)
- Emotion labels

Emotions include:
- Angry
- Disgust
- Fear
- Happy
- Neutral
- Sad
- Surprise
(Depending on dataset version)

Place dataset inside:


---

## ğŸ—ï¸ Project Structure

project/
â”‚
â”œâ”€â”€ data/
â”‚
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ speech_pipeline/
â”‚ â”œâ”€â”€ text_pipeline/
â”‚ â””â”€â”€ fusion_pipeline/
â”‚
â”œâ”€â”€ Results/
â”‚ â”œâ”€â”€ speech_accuracy.csv
â”‚ â”œâ”€â”€ text_accuracy.csv
â”‚ â”œâ”€â”€ fusion_accuracy.csv
â”‚ â”œâ”€â”€ accuracy_comparison.png
â”‚ â””â”€â”€ plots/
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

---

## âš™ï¸ Architecture Design

### 1ï¸âƒ£ Speech Pipeline

- Preprocessing:
  - Resampling to 16kHz
  - Silence trimming
- Feature Extraction:
  - MFCC (40 coefficients)
- Temporal Modelling:
  - BiLSTM (Bidirectional LSTM)
- Classifier:
  - Fully Connected layer

---

### 2ï¸âƒ£ Text Pipeline

- Text extraction from filename
- Tokenization using BERT tokenizer
- Contextual Modelling:
  - Pretrained BERT (bert-base-uncased)
- Classifier:
  - Linear layer on CLS token

---

### 3ï¸âƒ£ Fusion Pipeline

- Speech embedding (256-dim)
- Text embedding (768-dim)
- Concatenation (1024-dim)
- Fully Connected classifier

Fusion learns joint representation from both modalities.

---

## ğŸš€ How to Run

### 1ï¸âƒ£ Install dependencies

pip install -r requirements.txt

---

### 2ï¸âƒ£ Train Models

Speech:
cd models/speech_pipeline
python train.py
Text:
cd models/text_pipeline
python train.py

Fusion:
cd models/fusion_pipeline
python train.py

---

### 3ï¸âƒ£ Test Models

Speech:
python test.py

Text:
python test.py

Fusion:
python test.py

Accuracy Comparison:
cd Results
python plot_accuracy_comparison.py

---

## ğŸ“Š Results

The system evaluates:

- Test Accuracy
- Confusion Matrix
- Accuracy comparison bar plot

### Observations

- Text model performs strongly due to contextual understanding from BERT.
- Speech model captures emotional tone patterns.
- Fusion improves performance by combining acoustic and semantic information.
- Fusion particularly improves classification for subtle emotions.

---

## ğŸ“ˆ Visualization

Generated:

- Speech confusion matrix
- Text confusion matrix
- Fusion confusion matrix
- Accuracy comparison plot

Fusion embeddings show better class separability.

---

## ğŸ§  Key Insights

- Speech captures prosody and tone.
- Text captures semantic meaning.
- Fusion helps when one modality is ambiguous.
- Hardest emotions: Fear vs Surprise (similar acoustic features)
- Easiest emotions: Angry, Happy (distinct patterns)

---

## ğŸ› ï¸ Libraries Used

- PyTorch
- Transformers (HuggingFace)
- Librosa
- Scikit-learn
- Matplotlib
- Seaborn

---

